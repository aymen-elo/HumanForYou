{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e87b8a6",
   "metadata": {},
   "source": [
    "# 01 — Data Validation & Merge Pipeline\n",
    "## HumanForYou — Employee Attrition Prediction\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "Establish a **reproducible and traceable workflow** to:\n",
    "1. Validate the integrity of all 4 source datasets (general_data, employee_survey, manager_survey, badge data)\n",
    "2. Check schema consistency, missing values, and data quality\n",
    "3. Merge into a single analysis-ready DataFrame\n",
    "4. Export the clean dataset for downstream notebooks\n",
    "\n",
    "> **Adapted from** the pipeline architecture of a previous YOLO detection project — same rigor, different domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f33ead",
   "metadata": {},
   "source": [
    "## Section 1: Configuration and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2d710e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded — 2026-02-19 10:40\n",
      "Project root   : c:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\n",
      "Data directory  : c:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\\data\\raw\n",
      "Output directory: c:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\\outputs\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION — Centralized paths and parameters\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Only suppress expected warnings, not real errors\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# --- Path Configuration ---\n",
    "# Detect project root reliably (works regardless of CWD)\n",
    "_cwd = Path.cwd()\n",
    "if (_cwd / \"data\" / \"raw\").exists():\n",
    "    PROJECT_ROOT = _cwd                          # CWD is project root\n",
    "elif (_cwd.parent / \"data\" / \"raw\").exists():\n",
    "    PROJECT_ROOT = _cwd.parent                   # CWD is notebooks/\n",
    "else:\n",
    "    # Fallback: hardcoded absolute path\n",
    "    PROJECT_ROOT = Path(r\"c:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\")\n",
    "\n",
    "DATA_DIR   = str(PROJECT_ROOT / \"data\" / \"raw\")\n",
    "OUTPUT_DIR = str(PROJECT_ROOT / \"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Source files declaration\n",
    "FILES = {\n",
    "    \"general\":         os.path.join(DATA_DIR, \"general_data.csv\"),\n",
    "    \"employee_survey\": os.path.join(DATA_DIR, \"employee_survey_data.csv\"),\n",
    "    \"manager_survey\":  os.path.join(DATA_DIR, \"manager_survey_data.csv\"),\n",
    "    \"in_time\":         os.path.join(DATA_DIR, \"in_time.csv\"),\n",
    "    \"out_time\":        os.path.join(DATA_DIR, \"out_time.csv\"),\n",
    "}\n",
    "\n",
    "EMPLOYEE_ID_COL = \"EmployeeID\"\n",
    "\n",
    "print(f\"Configuration loaded — {datetime.now():%Y-%m-%d %H:%M}\")\n",
    "print(f\"Project root   : {PROJECT_ROOT}\")\n",
    "print(f\"Data directory  : {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a06b14",
   "metadata": {},
   "source": [
    "## Section 2: File Existence & Schema Validation\n",
    "\n",
    "**Purpose**: Verify every source file exists, is non-empty, and has the expected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe38d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "FILE VALIDATION\n",
      "=================================================================\n",
      "  [OK]   general              —  4410 rows ×  24 cols  (537 KB)\n",
      "  [OK]   employee_survey      —  4410 rows ×   4 cols  (51 KB)\n",
      "  [OK]   manager_survey       —  4410 rows ×   3 cols  (42 KB)\n",
      "  [OK]   in_time              —  4410 rows × 262 cols  (22,741 KB)\n",
      "  [OK]   out_time             —  4410 rows × 262 cols  (22,741 KB)\n",
      "\n",
      "Loaded 5/5 files successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FILE EXISTENCE CHECK\n",
    "# ==============================================================================\n",
    "\n",
    "def validate_file(name, path):\n",
    "    \"\"\"Check file exists and is non-empty. Return row/col counts.\"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        print(f\"  [FAIL] {name}: file not found at {path}\")\n",
    "        return None\n",
    "    size_kb = os.path.getsize(path) / 1024\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"  [OK]   {name:20s} — {df.shape[0]:>5} rows × {df.shape[1]:>3} cols  ({size_kb:,.0f} KB)\")\n",
    "    return df\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"FILE VALIDATION\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "raw = {}\n",
    "for key, path in FILES.items():\n",
    "    result = validate_file(key, path)\n",
    "    if result is not None:\n",
    "        raw[key] = result\n",
    "\n",
    "print(f\"\\nLoaded {len(raw)}/{len(FILES)} files successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557c8e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHEMA VALIDATION\n",
      "=================================================================\n",
      "  [OK] general\n",
      "  [OK] employee_survey\n",
      "  [OK] manager_survey\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SCHEMA VALIDATION — Expected columns\n",
    "# ==============================================================================\n",
    "\n",
    "EXPECTED_SCHEMAS = {\n",
    "    \"general\": [\n",
    "        \"Age\", \"Attrition\", \"BusinessTravel\", \"Department\", \"DistanceFromHome\",\n",
    "        \"Education\", \"EducationField\", \"EmployeeCount\", \"EmployeeID\", \"Gender\",\n",
    "        \"JobLevel\", \"JobRole\", \"MaritalStatus\", \"MonthlyIncome\",\n",
    "        \"NumCompaniesWorked\", \"Over18\", \"PercentSalaryHike\", \"StandardHours\",\n",
    "        \"StockOptionLevel\", \"TotalWorkingYears\", \"TrainingTimesLastYear\",\n",
    "        \"YearsAtCompany\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\"\n",
    "    ],\n",
    "    \"employee_survey\": [\"EmployeeID\", \"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\"],\n",
    "    \"manager_survey\":  [\"EmployeeID\", \"JobInvolvement\", \"PerformanceRating\"],\n",
    "}\n",
    "\n",
    "print(\"SCHEMA VALIDATION\")\n",
    "print(\"=\" * 65)\n",
    "for key, expected_cols in EXPECTED_SCHEMAS.items():\n",
    "    actual = list(raw[key].columns)\n",
    "    missing = set(expected_cols) - set(actual)\n",
    "    extra   = set(actual) - set(expected_cols)\n",
    "    status = \"OK\" if not missing else \"FAIL\"\n",
    "    print(f\"  [{status}] {key}\")\n",
    "    if missing:\n",
    "        print(f\"         Missing columns: {missing}\")\n",
    "    if extra:\n",
    "        print(f\"         Extra columns  : {extra}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484068e",
   "metadata": {},
   "source": [
    "## Section 3: Data Quality Audit\n",
    "\n",
    "**Purpose**: For each dataset, check missing values, duplicates, constant columns, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b80f363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING VALUES AUDIT\n",
      "=================================================================\n",
      "  general: 2 column(s) with missing values\n",
      "    → NumCompaniesWorked: 19 (0.4%)\n",
      "    → TotalWorkingYears: 9 (0.2%)\n",
      "  employee_survey: 3 column(s) with missing values\n",
      "    → WorkLifeBalance: 38 (0.9%)\n",
      "    → EnvironmentSatisfaction: 25 (0.6%)\n",
      "    → JobSatisfaction: 20 (0.5%)\n",
      "  manager_survey: No missing values ✓\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MISSING VALUES REPORT\n",
    "# ==============================================================================\n",
    "\n",
    "def missing_report(df, name):\n",
    "    \"\"\"Print missing value summary for a DataFrame.\"\"\"\n",
    "    total = df.isnull().sum()\n",
    "    pct   = (total / len(df) * 100).round(2)\n",
    "    report = pd.DataFrame({\"missing\": total, \"pct\": pct})\n",
    "    report = report[report[\"missing\"] > 0].sort_values(\"pct\", ascending=False)\n",
    "    if report.empty:\n",
    "        print(f\"  {name}: No missing values ✓\")\n",
    "    else:\n",
    "        print(f\"  {name}: {len(report)} column(s) with missing values\")\n",
    "        for col, row in report.iterrows():\n",
    "            print(f\"    → {col}: {int(row['missing'])} ({row['pct']:.1f}%)\")\n",
    "    return report\n",
    "\n",
    "print(\"MISSING VALUES AUDIT\")\n",
    "print(\"=\" * 65)\n",
    "missing_reports = {}\n",
    "for key, df in raw.items():\n",
    "    if key not in (\"in_time\", \"out_time\"):  # badge data handled separately\n",
    "        missing_reports[key] = missing_report(df, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8769c0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUPLICATE EMPLOYEES CHECK\n",
      "=================================================================\n",
      "  general: 0 duplicate EmployeeIDs ✓\n",
      "  employee_survey: 0 duplicate EmployeeIDs ✓\n",
      "  manager_survey: 0 duplicate EmployeeIDs ✓\n",
      "\n",
      "CONSTANT COLUMNS CHECK\n",
      "=================================================================\n",
      "  Constant columns found (candidates for removal): ['EmployeeCount', 'Over18', 'StandardHours']\n",
      "    → EmployeeCount: unique value = [1]\n",
      "    → Over18: unique value = <StringArray>\n",
      "['Y']\n",
      "Length: 1, dtype: str\n",
      "    → StandardHours: unique value = [8]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DUPLICATE & CONSTANT COLUMN CHECK\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"DUPLICATE EMPLOYEES CHECK\")\n",
    "print(\"=\" * 65)\n",
    "for key in [\"general\", \"employee_survey\", \"manager_survey\"]:\n",
    "    df = raw[key]\n",
    "    n_dup = df[EMPLOYEE_ID_COL].duplicated().sum()\n",
    "    print(f\"  {key}: {n_dup} duplicate EmployeeIDs {'✓' if n_dup == 0 else '⚠'}\")\n",
    "\n",
    "print(\"\\nCONSTANT COLUMNS CHECK\")\n",
    "print(\"=\" * 65)\n",
    "df_gen = raw[\"general\"]\n",
    "constant_cols = [col for col in df_gen.columns if df_gen[col].nunique() <= 1]\n",
    "if constant_cols:\n",
    "    print(f\"  Constant columns found (candidates for removal): {constant_cols}\")\n",
    "    for col in constant_cols:\n",
    "        print(f\"    → {col}: unique value = {df_gen[col].unique()}\")\n",
    "else:\n",
    "    print(\"  No constant columns found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922124b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMPLOYEE ID CROSS-FILE CONSISTENCY\n",
      "=================================================================\n",
      "  general              ↔ general: Perfect match (4410 IDs) ✓\n",
      "  employee_survey      ↔ general: Perfect match (4410 IDs) ✓\n",
      "  manager_survey       ↔ general: Perfect match (4410 IDs) ✓\n",
      "  in_time              ↔ general: Perfect match (4410 IDs) ✓\n",
      "  out_time             ↔ general: Perfect match (4410 IDs) ✓\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EMPLOYEE ID CONSISTENCY ACROSS DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"EMPLOYEE ID CROSS-FILE CONSISTENCY\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "ids_general  = set(raw[\"general\"][EMPLOYEE_ID_COL])\n",
    "ids_employee = set(raw[\"employee_survey\"][EMPLOYEE_ID_COL])\n",
    "ids_manager  = set(raw[\"manager_survey\"][EMPLOYEE_ID_COL])\n",
    "\n",
    "# in_time / out_time: first column is EmployeeID (unnamed)\n",
    "ids_in  = set(raw[\"in_time\"].iloc[:, 0].astype(int))\n",
    "ids_out = set(raw[\"out_time\"].iloc[:, 0].astype(int))\n",
    "\n",
    "all_sets = {\n",
    "    \"general\": ids_general, \"employee_survey\": ids_employee,\n",
    "    \"manager_survey\": ids_manager, \"in_time\": ids_in, \"out_time\": ids_out\n",
    "}\n",
    "\n",
    "ref = ids_general\n",
    "for name, s in all_sets.items():\n",
    "    only_ref  = ref - s\n",
    "    only_this = s - ref\n",
    "    if not only_ref and not only_this:\n",
    "        print(f\"  {name:20s} ↔ general: Perfect match ({len(s)} IDs) ✓\")\n",
    "    else:\n",
    "        print(f\"  {name:20s} ↔ general: {len(only_ref)} missing, {len(only_this)} extra ⚠\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9561b5",
   "metadata": {},
   "source": [
    "## Section 4: Badge Data Processing\n",
    "\n",
    "**Purpose**: Transform raw badge timestamps (in_time / out_time) into meaningful features per employee:\n",
    "- Average arrival & departure hours\n",
    "- Average daily working hours\n",
    "- Absence rate (% of working days with no badge)\n",
    "- Punctuality indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a6a45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing badge data (vectorized)...\n",
      "Badge features computed for 4410 employees.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>avg_arrival_hour</th>\n",
       "      <th>avg_departure_hour</th>\n",
       "      <th>avg_working_hours</th>\n",
       "      <th>absence_rate</th>\n",
       "      <th>late_arrival_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4410.000</td>\n",
       "      <td>4410.000</td>\n",
       "      <td>4410.000</td>\n",
       "      <td>4410.000</td>\n",
       "      <td>4410.000</td>\n",
       "      <td>4410.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2205.500</td>\n",
       "      <td>9.992</td>\n",
       "      <td>17.693</td>\n",
       "      <td>7.701</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1273.202</td>\n",
       "      <td>0.018</td>\n",
       "      <td>1.340</td>\n",
       "      <td>1.340</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>9.929</td>\n",
       "      <td>15.942</td>\n",
       "      <td>5.951</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1103.250</td>\n",
       "      <td>9.980</td>\n",
       "      <td>16.656</td>\n",
       "      <td>6.673</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2205.500</td>\n",
       "      <td>9.992</td>\n",
       "      <td>17.400</td>\n",
       "      <td>7.407</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3307.750</td>\n",
       "      <td>10.004</td>\n",
       "      <td>18.352</td>\n",
       "      <td>8.368</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4410.000</td>\n",
       "      <td>10.070</td>\n",
       "      <td>21.059</td>\n",
       "      <td>11.031</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       EmployeeID  avg_arrival_hour  avg_departure_hour  avg_working_hours  \\\n",
       "count    4410.000          4410.000            4410.000           4410.000   \n",
       "mean     2205.500             9.992              17.693              7.701   \n",
       "std      1273.202             0.018               1.340              1.340   \n",
       "min         1.000             9.929              15.942              5.951   \n",
       "25%      1103.250             9.980              16.656              6.673   \n",
       "50%      2205.500             9.992              17.400              7.407   \n",
       "75%      3307.750            10.004              18.352              8.368   \n",
       "max      4410.000            10.070              21.059             11.031   \n",
       "\n",
       "       absence_rate  late_arrival_rate  \n",
       "count      4410.000           4410.000  \n",
       "mean          0.095              0.500  \n",
       "std           0.021              0.033  \n",
       "min           0.050              0.385  \n",
       "25%           0.077              0.477  \n",
       "50%           0.096              0.500  \n",
       "75%           0.111              0.522  \n",
       "max           0.138              0.627  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# BADGE DATA — Vectorized processing (fast)\n",
    "# ==============================================================================\n",
    "\n",
    "import gc\n",
    "\n",
    "def process_badge_data(df_in, df_out):\n",
    "    \"\"\"\n",
    "    Transform raw badge in/out timestamps into employee-level features.\n",
    "    Fully vectorized — processes all employees in seconds.\n",
    "    \n",
    "    Returns a DataFrame indexed by EmployeeID with:\n",
    "      - avg_arrival_hour, avg_departure_hour, avg_working_hours\n",
    "      - absence_rate (fraction of working days with NA badge)\n",
    "      - late_arrival_rate (fraction of days arriving after 10:00)\n",
    "    \"\"\"\n",
    "    emp_ids = df_in.iloc[:, 0].astype(int)\n",
    "    \n",
    "    # Extract only date columns (skip EmployeeID column)\n",
    "    in_dates = df_in.iloc[:, 1:]\n",
    "    out_dates = df_out.iloc[:, 1:]\n",
    "    \n",
    "    # Convert all timestamps at once (NaN/NA become NaT)\n",
    "    in_parsed = in_dates.apply(pd.to_datetime, errors=\"coerce\")\n",
    "    out_parsed = out_dates.apply(pd.to_datetime, errors=\"coerce\")\n",
    "    \n",
    "    # Extract hours as float (hour + minute/60)\n",
    "    in_hours = in_parsed.apply(lambda col: col.dt.hour + col.dt.minute / 60)\n",
    "    out_hours = out_parsed.apply(lambda col: col.dt.hour + col.dt.minute / 60)\n",
    "    \n",
    "    # Free parsed datetime DataFrames (no longer needed)\n",
    "    del in_parsed, out_parsed, in_dates, out_dates\n",
    "    gc.collect()\n",
    "    \n",
    "    # Working hours per day\n",
    "    work_hours = out_hours - in_hours\n",
    "    \n",
    "    # Count valid days (not NaN) per employee\n",
    "    n_days = in_hours.shape[1]\n",
    "    valid_mask = in_hours.notna() & out_hours.notna()\n",
    "    n_present = valid_mask.sum(axis=1)\n",
    "    n_absent = n_days - n_present\n",
    "    \n",
    "    # Late arrivals (after 10:00)\n",
    "    late_mask = valid_mask & (in_hours >= 10)\n",
    "    n_late = late_mask.sum(axis=1)\n",
    "    \n",
    "    # Compute aggregates\n",
    "    avg_arrival = in_hours.mean(axis=1)\n",
    "    avg_departure = out_hours.mean(axis=1)\n",
    "    avg_work = work_hours.where(valid_mask).mean(axis=1)\n",
    "    \n",
    "    # Free large intermediate DataFrames\n",
    "    del in_hours, out_hours, work_hours, valid_mask, late_mask\n",
    "    gc.collect()\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        EMPLOYEE_ID_COL: emp_ids.values,\n",
    "        \"avg_arrival_hour\":   avg_arrival,\n",
    "        \"avg_departure_hour\": avg_departure,\n",
    "        \"avg_working_hours\":  avg_work,\n",
    "        \"absence_rate\":       n_absent / n_days,\n",
    "        \"late_arrival_rate\":  (n_late / n_present).where(n_present > 0),\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Processing badge data (vectorized)...\")\n",
    "df_badge = process_badge_data(raw[\"in_time\"], raw[\"out_time\"])\n",
    "\n",
    "# Free raw badge data — no longer needed\n",
    "del raw[\"in_time\"], raw[\"out_time\"]\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Badge features computed for {len(df_badge)} employees.\")\n",
    "df_badge.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b2434",
   "metadata": {},
   "source": [
    "## Section 5: Dataset Merge\n",
    "\n",
    "**Purpose**: Inner-join all datasets on EmployeeID to create the unified analysis DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0417cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset: 4410 rows x 31 columns\n",
      "Target distribution (Attrition):\n",
      "Attrition\n",
      "No     3699\n",
      "Yes     711\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attrition rate: 16.1%\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MERGE ALL DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "df = raw[\"general\"].copy()\n",
    "\n",
    "# Merge survey data\n",
    "df = df.merge(raw[\"employee_survey\"], on=EMPLOYEE_ID_COL, how=\"left\")\n",
    "df = df.merge(raw[\"manager_survey\"],  on=EMPLOYEE_ID_COL, how=\"left\")\n",
    "df = df.merge(df_badge,               on=EMPLOYEE_ID_COL, how=\"left\")\n",
    "\n",
    "# Drop constant / uninformative columns identified earlier\n",
    "cols_to_drop = [\"EmployeeCount\", \"Over18\", \"StandardHours\"]\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "print(f\"Merged dataset: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "print(f\"Target distribution (Attrition):\")\n",
    "print(df[\"Attrition\"].value_counts())\n",
    "print(f\"\\nAttrition rate: {(df['Attrition'] == 'Yes').mean() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0f6199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST-MERGE QUALITY CHECK\n",
      "=================================================================\n",
      "  5 columns with missing values:\n",
      "    → NumCompaniesWorked: 19 (0.4%)\n",
      "    → TotalWorkingYears: 9 (0.2%)\n",
      "    → EnvironmentSatisfaction: 25 (0.6%)\n",
      "    → JobSatisfaction: 20 (0.5%)\n",
      "    → WorkLifeBalance: 38 (0.9%)\n",
      "\n",
      "Column types:\n",
      "  Numeric : 24\n",
      "  Object  : 7\n",
      "  Total   : 31\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FINAL QUALITY CHECK ON MERGED DATA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"POST-MERGE QUALITY CHECK\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Missing values in merged dataset\n",
    "total_missing = df.isnull().sum()\n",
    "cols_with_na = total_missing[total_missing > 0]\n",
    "if cols_with_na.empty:\n",
    "    print(\"  No missing values in merged dataset ✓\")\n",
    "else:\n",
    "    print(f\"  {len(cols_with_na)} columns with missing values:\")\n",
    "    for col, count in cols_with_na.items():\n",
    "        print(f\"    → {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Data types summary\n",
    "print(f\"\\nColumn types:\")\n",
    "print(f\"  Numeric : {df.select_dtypes(include='number').shape[1]}\")\n",
    "print(f\"  Object  : {df.select_dtypes(include='object').shape[1]}\")\n",
    "print(f\"  Total   : {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8bac84",
   "metadata": {},
   "source": [
    "## Section 6: Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b59c72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset exported to: c:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\\outputs\\merged_data.csv\n",
      "Shape: (4410, 31)\n",
      "Summary saved to: c:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\\outputs\\data_validation_summary.txt\n",
      "\n",
      "✓ Pipeline complete — proceed to 02_EDA_Explorer.ipynb\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EXPORT CLEAN DATASET\n",
    "# ==============================================================================\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, \"merged_data.csv\")\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Merged dataset exported to: {os.path.abspath(output_path)}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Also save a quick summary\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"data_validation_summary.txt\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(f\"Data Validation Summary — {datetime.now():%Y-%m-%d %H:%M}\\n\")\n",
    "    f.write(f\"{'='*50}\\n\")\n",
    "    f.write(f\"Total employees: {len(df)}\\n\")\n",
    "    f.write(f\"Total features:  {df.shape[1]}\\n\")\n",
    "    f.write(f\"Attrition rate:  {(df['Attrition']=='Yes').mean()*100:.1f}%\\n\")\n",
    "    f.write(f\"Columns dropped: {cols_to_drop}\\n\")\n",
    "    f.write(f\"Badge features added: avg_arrival_hour, avg_departure_hour, avg_working_hours, absence_rate, late_arrival_rate\\n\")\n",
    "\n",
    "print(f\"Summary saved to: {os.path.abspath(summary_path)}\")\n",
    "print(\"\\n✓ Pipeline complete — proceed to 02_EDA_Explorer.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378211ab",
   "metadata": {},
   "source": [
    "## Section 7: Data Leakage Diagnostic — Badge Features\n",
    "\n",
    "**Purpose**: Verify that badge-derived features (avg_working_hours, absence_rate, etc.) do not encode attrition that already occurred during 2015. If employees left mid-year, their badge metrics would reflect their departure rather than predict it.\n",
    "\n",
    "> **ALTAI Requirement 2 (Robustness)**: Model validity depends on features being genuinely predictive, not tautological."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d66e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA LEAKAGE DIAGNOSTIC — Badge Features vs Attrition\n",
    "# ==============================================================================\n",
    "# If employees left mid-2015, their badge metrics (absence_rate, avg_working_hours)\n",
    "# would reflect their departure, not predict it. This cell checks for that.\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "merged = pd.read_csv(os.path.join(OUTPUT_DIR, \"merged_data.csv\"))\n",
    "attrition_yes = merged[merged[\"Attrition\"] == \"Yes\"]\n",
    "attrition_no  = merged[merged[\"Attrition\"] == \"No\"]\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"DATA LEAKAGE DIAGNOSTIC — Badge Features vs Attrition\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "badge_cols = [\"absence_rate\", \"avg_working_hours\", \"avg_departure_hour\",\n",
    "              \"avg_arrival_hour\", \"late_arrival_rate\"]\n",
    "\n",
    "leakage_alerts = []\n",
    "for col in badge_cols:\n",
    "    if col not in merged.columns:\n",
    "        continue\n",
    "    yes_vals = attrition_yes[col].dropna()\n",
    "    no_vals  = attrition_no[col].dropna()\n",
    "    if len(yes_vals) == 0 or len(no_vals) == 0:\n",
    "        continue\n",
    "\n",
    "    u_stat, p_val = stats.mannwhitneyu(yes_vals, no_vals, alternative=\"two-sided\")\n",
    "    effect_size = yes_vals.mean() - no_vals.mean()\n",
    "    std_pooled = merged[col].std()\n",
    "    cohens_d = effect_size / std_pooled if std_pooled > 0 else 0\n",
    "\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Mean (Attrition=Yes): {yes_vals.mean():.4f}\")\n",
    "    print(f\"  Mean (Attrition=No):  {no_vals.mean():.4f}\")\n",
    "    print(f\"  Δ = {effect_size:+.4f}  |  Cohen's d = {cohens_d:+.3f}  |  p = {p_val:.2e}\")\n",
    "\n",
    "    if p_val < 0.001 and abs(cohens_d) > 0.5:\n",
    "        print(f\"  ⚠️  WARNING: Potential data leakage — large effect + high significance\")\n",
    "        leakage_alerts.append(col)\n",
    "\n",
    "if leakage_alerts:\n",
    "    print(f\"\\n{'='*65}\")\n",
    "    print(f\"⚠️  {len(leakage_alerts)} feature(s) flagged for potential leakage:\")\n",
    "    for col in leakage_alerts:\n",
    "        print(f\"   - {col}\")\n",
    "    print(\"\\n   These features may encode attrition that already occurred during 2015.\")\n",
    "    print(\"   → Run the ablation test in Notebook 05 (model without badge features)\")\n",
    "    print(\"   → Consider restricting badge data to Jan–Jun 2015 only\")\n",
    "else:\n",
    "    print(f\"\\n✅ No strong leakage signal detected in badge features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
