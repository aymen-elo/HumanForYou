{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Feature Engineering & Preprocessing",
    "## HumanForYou — Employee Attrition Prediction",
    "",
    "---",
    "",
    "### Objective",
    "",
    "Transform raw merged data into **model-ready features**:",
    "1. Handle missing values with justified strategies",
    "2. Encode categorical variables (ordinal vs. one-hot)",
    "3. Engineer new features from existing ones",
    "4. Scale numerical features",
    "5. Address class imbalance (SMOTE)",
    "6. Export train/test splits for modeling",
    "",
    "> This notebook expects `merged_data.csv` from **01_Data_Validation_Pipeline**."
   ],
   "id": "4a41c888"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup"
   ],
   "id": "ef674950"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# ==============================================================================\n# IMPORTS\n# ==============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\n\n# --- Path Configuration (same logic as notebook 01) ---\n_cwd = Path.cwd()\nif (_cwd / \"data\" / \"raw\").exists():\n    PROJECT_ROOT = _cwd\nelif (_cwd.parent / \"data\" / \"raw\").exists():\n    PROJECT_ROOT = _cwd.parent\nelse:\n    PROJECT_ROOT = Path(r\"c:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\")\n\nOUTPUT_DIR = str(PROJECT_ROOT / \"outputs\")\n\ndf = pd.read_csv(f\"{OUTPUT_DIR}/merged_data.csv\")\n\n# Binary target\ndf[\"Attrition\"] = (df[\"Attrition\"] == \"Yes\").astype(int)\n\nprint(f\"Loaded: {df.shape[0]} rows x {df.shape[1]} columns\")",
   "outputs": [],
   "id": "4a505977"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2: Feature Engineering & Encoding (before split)\n\nFeatures that don't require fitting on data (no leakage risk) are created before the split.\nImputation happens **after** the split to avoid data leakage.",
   "id": "ca26e532"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# ==============================================================================\n# FEATURE ENGINEERING (safe before split — no fitted statistics)\n# ==============================================================================\n\n# Income per job level\nif \"MonthlyIncome\" in df.columns and \"JobLevel\" in df.columns:\n    df[\"IncomePerJobLevel\"] = df[\"MonthlyIncome\"] / df[\"JobLevel\"]\n\n# Promotion stagnation\nif \"YearsSinceLastPromotion\" in df.columns and \"YearsAtCompany\" in df.columns:\n    df[\"PromotionStagnation\"] = df[\"YearsSinceLastPromotion\"] / (df[\"YearsAtCompany\"] + 1)\n\n# Satisfaction composite score\nsurvey_items = [\"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\"]\nexisting_items = [c for c in survey_items if c in df.columns]\nif existing_items:\n    df[\"SatisfactionScore\"] = df[existing_items].mean(axis=1)\n\n# Manager stability\nif \"YearsWithCurrManager\" in df.columns and \"YearsAtCompany\" in df.columns:\n    df[\"ManagerStability\"] = df[\"YearsWithCurrManager\"] / (df[\"YearsAtCompany\"] + 1)\n\n# Long hours proxy\nif \"avg_working_hours\" in df.columns:\n    df[\"LongHours\"] = (df[\"avg_working_hours\"] > 9).astype(int)\n\nnew_features = [\"IncomePerJobLevel\", \"PromotionStagnation\", \"SatisfactionScore\", \"ManagerStability\", \"LongHours\"]\nnew_features = [f for f in new_features if f in df.columns]\nprint(f\"New features created: {new_features}\")\n\n# ==============================================================================\n# CATEGORICAL ENCODING (deterministic — no leakage)\n# ==============================================================================\n\ncat_cols = df.select_dtypes(include=\"object\").columns.tolist()\nprint(f\"Categorical columns to encode: {cat_cols}\")\n\n# Ordinal encoding for BusinessTravel\nbt_map = {\"Non-Travel\": 0, \"Travel_Rarely\": 1, \"Travel_Frequently\": 2}\nif \"BusinessTravel\" in df.columns:\n    df[\"BusinessTravel\"] = df[\"BusinessTravel\"].map(bt_map)\n\n# One-hot encoding for remaining categoricals\nohe_cols = [c for c in cat_cols if c != \"BusinessTravel\"]\ndf = pd.get_dummies(df, columns=ohe_cols, drop_first=True, dtype=int)\n\nprint(f\"Post-encoding shape: {df.shape[0]} rows x {df.shape[1]} columns\")",
   "outputs": [],
   "id": "60770412"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3: Train / Test Split\n\nSplit **before** imputation to prevent data leakage.",
   "id": "bf719e6c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# ==============================================================================\n# TRAIN / TEST SPLIT (before imputation to avoid leakage)\n# ==============================================================================\n\n# Save and drop EmployeeID\nif \"EmployeeID\" in df.columns:\n    employee_ids = df[\"EmployeeID\"].copy()\n    df = df.drop(columns=[\"EmployeeID\"])\n\nX = df.drop(columns=[\"Attrition\"])\ny = df[\"Attrition\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Train set: {X_train.shape[0]} samples ({y_train.mean()*100:.1f}% attrition)\")\nprint(f\"Test set:  {X_test.shape[0]} samples ({y_test.mean()*100:.1f}% attrition)\")\nprint(f\"Missing in train: {X_train.isnull().sum().sum()}\")\nprint(f\"Missing in test:  {X_test.isnull().sum().sum()}\")",
   "outputs": [],
   "id": "47243ef7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 4: Imputation & Scaling (fit on train only)\n\n**Strategy**:\n- Median imputation for all columns with missing values (robust to outliers)\n- **Fit on train, transform both** — no leakage",
   "id": "7756702d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# ==============================================================================\n# IMPUTATION — fit on train only, transform both\n# ==============================================================================\n\n# Identify columns with missing values\ncols_with_na = X_train.columns[X_train.isnull().any()].tolist()\nprint(f\"Columns to impute: {cols_with_na}\")\n\n# Median imputer — fit on train only\nimputer = SimpleImputer(strategy=\"median\")\nX_train[cols_with_na] = imputer.fit_transform(X_train[cols_with_na])\nX_test[cols_with_na] = imputer.transform(X_test[cols_with_na])\n\nprint(f\"Imputation complete (median, fit on train only)\")\nprint(f\"Remaining NaN — train: {X_train.isnull().sum().sum()}, test: {X_test.isnull().sum().sum()}\")\n\n# ==============================================================================\n# SCALING — fit on train only\n# ==============================================================================\n\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\nX_test_scaled  = pd.DataFrame(scaler.transform(X_test),      columns=X_test.columns,  index=X_test.index)\n\nprint(\"Scaling applied (StandardScaler — fit on train only).\")",
   "outputs": [],
   "id": "91c646e0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 5: Class Imbalance — SMOTE\n\nApply SMOTE **only on the training set** to avoid data leakage.",
   "id": "b665dbdd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# ==============================================================================\n# SMOTE OVERSAMPLING (train set only)\n# ==============================================================================\n\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n\nprint(f\"Before SMOTE: {y_train.value_counts().to_dict()}\")\nprint(f\"After  SMOTE: {pd.Series(y_train_resampled).value_counts().to_dict()}\")",
   "outputs": [],
   "id": "68fb106e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "execution_count": null,
   "source": "## Section 6: Export Preprocessed Data",
   "outputs": [],
   "id": "6e876435"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ==============================================================================\n# EXPORT\n# ==============================================================================\nimport joblib\n\n# Save processed data\nX_train_resampled.to_csv(f\"{OUTPUT_DIR}/X_train.csv\", index=False)\nX_test_scaled.to_csv(f\"{OUTPUT_DIR}/X_test.csv\", index=False)\npd.Series(y_train_resampled, name=\"Attrition\").to_csv(f\"{OUTPUT_DIR}/y_train.csv\", index=False)\ny_test.to_csv(f\"{OUTPUT_DIR}/y_test.csv\", index=False)\n\n# Also save non-SMOTE versions for fairness analysis and honest CV\nX_train_scaled.to_csv(f\"{OUTPUT_DIR}/X_train_no_smote.csv\", index=False)\ny_train.to_csv(f\"{OUTPUT_DIR}/y_train_no_smote.csv\", index=False)\n\n# Save pre-scaling train/test for fairness (unscaled binary columns)\nX_train.to_csv(f\"{OUTPUT_DIR}/X_train_unscaled.csv\", index=False)\nX_test.to_csv(f\"{OUTPUT_DIR}/X_test_unscaled.csv\", index=False)\n\n# Save scaler and imputer for reproducibility\njoblib.dump(scaler, f\"{OUTPUT_DIR}/scaler.joblib\")\njoblib.dump(imputer, f\"{OUTPUT_DIR}/imputer.joblib\")\n\n# Save feature names\nfeature_names = list(X_train.columns)\npd.Series(feature_names).to_csv(f\"{OUTPUT_DIR}/feature_names.csv\", index=False, header=False)\n\nprint(f\"Exported to {OUTPUT_DIR}/:\")\nprint(f\"  X_train.csv              ({X_train_resampled.shape}) — SMOTE + scaled\")\nprint(f\"  X_test.csv               ({X_test_scaled.shape}) — scaled\")\nprint(f\"  X_train_unscaled.csv     ({X_train.shape}) — for fairness analysis\")\nprint(f\"  X_test_unscaled.csv      ({X_test.shape}) — for fairness analysis\")\nprint(f\"  y_train.csv / y_test.csv\")\nprint(f\"  scaler.joblib / imputer.joblib\")\nprint(f\"  feature_names.csv        ({len(feature_names)} features)\")\nprint(\"\\nPipeline: split -> impute (fit train) -> scale (fit train) -> SMOTE (train only)\")\nprint(\"No data leakage.\")\nprint(\"\\n-> Proceed to 04_Model_Benchmark.ipynb\")",
   "id": "5f12d22f"
  }
 ]
}