{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a41c888",
   "metadata": {},
   "source": [
    "# 03 — Feature Engineering & Preprocessing\n",
    "## HumanForYou — Employee Attrition Prediction\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "Transform raw merged data into **model-ready features**:\n",
    "1. Handle missing values with justified strategies\n",
    "2. Encode categorical variables (ordinal vs. one-hot)\n",
    "3. Engineer new features from existing ones\n",
    "4. Scale numerical features\n",
    "5. Address class imbalance (SMOTE)\n",
    "6. Export train/test splits for modeling\n",
    "\n",
    "> This notebook expects `merged_data.csv` from **01_Data_Validation_Pipeline**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef674950",
   "metadata": {},
   "source": [
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a505977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:38:18.390503100Z",
     "start_time": "2026-02-20T12:38:17.906769300Z"
    }
   },
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Only suppress expected warnings, not real errors\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Path Configuration (same logic as notebook 01) ---\n",
    "_cwd = Path.cwd()\n",
    "if (_cwd / \"data\" / \"raw\").exists():\n",
    "    PROJECT_ROOT = _cwd\n",
    "elif (_cwd.parent / \"data\" / \"raw\").exists():\n",
    "    PROJECT_ROOT = _cwd.parent\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot find project root: 'data/raw/' not found in CWD or parent. \"\n",
    "        \"Run this notebook from the project root or notebooks/ directory.\"\n",
    "    )\n",
    "\n",
    "OUTPUT_DIR = str(PROJECT_ROOT / \"outputs\")\n",
    "\n",
    "df = pd.read_csv(f\"{OUTPUT_DIR}/merged_data.csv\")\n",
    "\n",
    "# Binary target\n",
    "df[\"Attrition\"] = (df[\"Attrition\"] == \"Yes\").astype(int)\n",
    "\n",
    "print(f\"Loaded: {df.shape[0]} rows x {df.shape[1]} columns\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 4410 rows x 29 columns\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ca26e532",
   "metadata": {},
   "source": [
    "## Section 2: Feature Engineering & Encoding (before split)\n",
    "\n",
    "Features that don't require fitting on data (no leakage risk) are created before the split.\n",
    "Imputation happens **after** the split to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "id": "60770412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:38:18.448929Z",
     "start_time": "2026-02-20T12:38:18.410541800Z"
    }
   },
   "source": "# ==============================================================================\n# FEATURE ENGINEERING (safe before split — no fitted statistics)\n# ==============================================================================\n\n# Income per job level\nif \"MonthlyIncome\" in df.columns and \"JobLevel\" in df.columns:\n    df[\"IncomePerJobLevel\"] = df[\"MonthlyIncome\"] / df[\"JobLevel\"]\n\n# Promotion stagnation\nif \"YearsSinceLastPromotion\" in df.columns and \"YearsAtCompany\" in df.columns:\n    df[\"PromotionStagnation\"] = df[\"YearsSinceLastPromotion\"] / (df[\"YearsAtCompany\"] + 1)\n\n# Satisfaction composite score\nsurvey_items = [\"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\"]\nexisting_items = [c for c in survey_items if c in df.columns]\nif existing_items:\n    df[\"SatisfactionScore\"] = df[existing_items].mean(axis=1)\n\n# Manager stability\nif \"YearsWithCurrManager\" in df.columns and \"YearsAtCompany\" in df.columns:\n    df[\"ManagerStability\"] = df[\"YearsWithCurrManager\"] / (df[\"YearsAtCompany\"] + 1)\n\n# REMOVED: LongHours — directly derived from avg_working_hours (r=0.835).\n# Keeping both would triple-encode the same signal (avg_working_hours,\n# avg_departure_hour [already removed in NB01], and LongHours).\n\nnew_features = [\"IncomePerJobLevel\", \"PromotionStagnation\", \"SatisfactionScore\", \"ManagerStability\"]\nnew_features = [f for f in new_features if f in df.columns]\nprint(f\"New features created: {new_features}\")\n\n# ==============================================================================\n# CATEGORICAL ENCODING (deterministic — no leakage)\n# ==============================================================================\n\ncat_cols = df.select_dtypes(include=\"object\").columns.tolist()\nprint(f\"Categorical columns to encode: {cat_cols}\")\n\n# Ordinal encoding for BusinessTravel\nbt_map = {\"Non-Travel\": 0, \"Travel_Rarely\": 1, \"Travel_Frequently\": 2}\nif \"BusinessTravel\" in df.columns:\n    df[\"BusinessTravel\"] = df[\"BusinessTravel\"].map(bt_map)\n\n# One-hot encoding for remaining categoricals\nohe_cols = [c for c in cat_cols if c != \"BusinessTravel\"]\ndf = pd.get_dummies(df, columns=ohe_cols, drop_first=True, dtype=int)\n\nprint(f\"Post-encoding shape: {df.shape[0]} rows x {df.shape[1]} columns\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features created: ['IncomePerJobLevel', 'PromotionStagnation', 'SatisfactionScore', 'ManagerStability']\n",
      "Categorical columns to encode: ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']\n",
      "Post-encoding shape: 4410 rows x 46 columns\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "bf719e6c",
   "metadata": {},
   "source": [
    "## Section 3: Train / Test Split\n",
    "\n",
    "Split **before** imputation to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "id": "47243ef7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:38:18.486037600Z",
     "start_time": "2026-02-20T12:38:18.450924800Z"
    }
   },
   "source": [
    "# ==============================================================================\n",
    "# TRAIN / TEST SPLIT (before imputation to avoid leakage)\n",
    "# ==============================================================================\n",
    "\n",
    "# Save and drop EmployeeID\n",
    "if \"EmployeeID\" in df.columns:\n",
    "    employee_ids = df[\"EmployeeID\"].copy()\n",
    "    df = df.drop(columns=[\"EmployeeID\"])\n",
    "\n",
    "X = df.drop(columns=[\"Attrition\"])\n",
    "y = df[\"Attrition\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Avoid SettingWithCopyWarning: train_test_split returns views, not copies.\n",
    "# Without .copy(), in-place imputation may silently fail or corrupt data.\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({y_train.mean()*100:.1f}% attrition)\")\n",
    "print(f\"Test set:  {X_test.shape[0]} samples ({y_test.mean()*100:.1f}% attrition)\")\n",
    "print(f\"Missing in train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing in test:  {X_test.isnull().sum().sum()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3528 samples (16.1% attrition)\n",
      "Test set:  882 samples (16.1% attrition)\n",
      "Missing in train: 89\n",
      "Missing in test:  22\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "7756702d",
   "metadata": {},
   "source": [
    "## Section 4: Imputation (fit on train only)\n",
    "\n",
    "**Strategy**:\n",
    "- **KNN imputation** (k=5, distance-weighted) for all columns with missing values\n",
    "- Preserves local data structure and reduces imputation bias\n",
    "- Aligned with ethics document recommendation (avoid selection bias from median imputation)\n",
    "- **Fit on train only** → transform both train and test (no data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "id": "91c646e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:38:18.575004500Z",
     "start_time": "2026-02-20T12:38:18.488020700Z"
    }
   },
   "source": [
    "# ==============================================================================\n",
    "# IMPUTATION — KNNImputer (fit on train only, transform both)\n",
    "# ==============================================================================\n",
    "# Using KNNImputer instead of SimpleImputer(median) to:\n",
    "# - Preserve local data structure and reduce imputation bias\n",
    "# - Align with ethics document recommendation (avoid selection bias)\n",
    "\n",
    "# Identify columns with missing values\n",
    "cols_with_na = X_train.columns[X_train.isnull().any()].tolist()\n",
    "print(f\"Columns to impute: {cols_with_na}\")\n",
    "\n",
    "# KNNImputer — distance-weighted, fit on train only\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "X_train[cols_with_na] = imputer.fit_transform(X_train[cols_with_na])\n",
    "X_test[cols_with_na] = imputer.transform(X_test[cols_with_na])\n",
    "\n",
    "print(f\"✅ KNNImputer (k=5, distance-weighted) — consistent with ethics document\")\n",
    "print(f\"   Remaining NaN — train: {X_train.isnull().sum().sum()}, test: {X_test.isnull().sum().sum()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to impute: ['NumCompaniesWorked', 'TotalWorkingYears', 'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance']\n",
      "✅ KNNImputer (k=5, distance-weighted) — consistent with ethics document\n",
      "   Remaining NaN — train: 0, test: 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "b665dbdd",
   "metadata": {},
   "source": "## Section 4b: Feature Selection & Scaling\n\nPipeline:\n1. **Feature whitelist** — apply the 24-feature selection from ablation analysis  \n   (hybrid: |r| >= 0.03 with Attrition OR top 82% cumulative Gini importance)\n2. **Variance threshold** on **unscaled** data: remove near-constant features (variance < 0.01)\n3. **Correlation filter**: remove features with |r| > 0.90 to reduce multicollinearity\n4. **Scaling** (StandardScaler — fit on train only)\n\n> See `outputs/feature_selection_final.csv` for the full feature audit (47 → 24).  \n> Dropping 20+ noise features costs < 0.5% F1 (validated by 5-scenario ablation test)."
  },
  {
   "cell_type": "code",
   "id": "68fb106e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:38:18.633540900Z",
     "start_time": "2026-02-20T12:38:18.577005200Z"
    }
   },
   "source": "# ==============================================================================\n# FEATURE SELECTION & SCALING\n# ==============================================================================\n# Selection validated by ablation tests (see outputs/ablation_results.json):\n#   47 features → 24 features: Delta CV-F1 < 0.5%\n#   Method: hybrid (|r| >= 0.03 with Attrition) ∪ (top 82% Gini importance)\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nn_before = X_train.shape[1]\n\n# ── 1) Feature whitelist (from ablation analysis) ────────────────────────\nFINAL_FEATURES = [\n    # Badge H1 (2 features)\n    \"avg_working_hours\", \"late_arrival_rate\",\n    # HR core (12 features)\n    \"Age\", \"TotalWorkingYears\", \"YearsAtCompany\", \"MonthlyIncome\",\n    \"YearsWithCurrManager\", \"NumCompaniesWorked\", \"DistanceFromHome\",\n    \"PercentSalaryHike\", \"TrainingTimesLastYear\", \"YearsSinceLastPromotion\",\n    \"BusinessTravel\", \"MaritalStatus_Single\",\n    # Surveys (3 features)\n    \"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\",\n    # Derived (4 features)\n    \"IncomePerJobLevel\", \"ManagerStability\", \"SatisfactionScore\", \"PromotionStagnation\",\n    # OHE kept (3 features)\n    \"MaritalStatus_Married\", \"JobRole_Manufacturing Director\",\n    \"EducationField_Technical Degree\",\n]\n\navailable = [f for f in FINAL_FEATURES if f in X_train.columns]\ndropped = [f for f in X_train.columns if f not in available]\nX_train = X_train[available]\nX_test  = X_test[available]\nprint(f\"Feature whitelist applied: {n_before} -> {len(available)} features\")\nprint(f\"  Dropped ({len(dropped)}): {dropped[:10]}{'...' if len(dropped) > 10 else ''}\")\n\n# ── 2) Remove near-constant features (variance < 0.01, UNSCALED) ────────\nvar_selector = VarianceThreshold(threshold=0.01)\nvar_selector.fit(X_train)\nlow_var_cols = X_train.columns[~var_selector.get_support()].tolist()\n\nif low_var_cols:\n    print(f\"\\nLow-variance features removed ({len(low_var_cols)}): {low_var_cols}\")\n    X_train = X_train.drop(columns=low_var_cols)\n    X_test = X_test.drop(columns=low_var_cols)\nelse:\n    print(\"\\nNo low-variance features found.\")\n\n# ── 3) Correlation filter (|r| > 0.90) ──────────────────────────────────\ncorr_matrix = X_train.corr().abs()\nupper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nhigh_corr_cols = [col for col in upper_tri.columns if any(upper_tri[col] > 0.90)]\n\nif high_corr_cols:\n    print(f\"Highly correlated features removed ({len(high_corr_cols)}): {high_corr_cols}\")\n    for col in high_corr_cols:\n        partners = upper_tri.index[upper_tri[col] > 0.90].tolist()\n        for p in partners:\n            print(f\"    {col} <-> {p}: r = {corr_matrix.loc[p, col]:.4f}\")\n    X_train = X_train.drop(columns=high_corr_cols)\n    X_test = X_test.drop(columns=high_corr_cols)\nelse:\n    print(\"No highly correlated feature pairs found (threshold: 0.90).\")\n\n# ── 4) Scaling — fit on train only ──────────────────────────────────────\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\nX_test_scaled  = pd.DataFrame(scaler.transform(X_test),      columns=X_test.columns,  index=X_test.index)\n\nn_after = X_train_scaled.shape[1]\nprint(f\"\\nScaling applied (StandardScaler — fit on train only).\")\nprint(f\"Final pipeline: {n_before} -> {n_after} features\")\nprint(f\"\\nFinal feature list ({n_after}):\")\nfor i, col in enumerate(X_train_scaled.columns, 1):\n    print(f\"  {i:2d}. {col}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature whitelist applied: 44 -> 24 features\n",
      "  Dropped (20): ['Education', 'JobLevel', 'StockOptionLevel', 'JobInvolvement', 'PerformanceRating', 'absence_rate', 'Department_Research & Development', 'Department_Sales', 'EducationField_Life Sciences', 'EducationField_Marketing']...\n",
      "\n",
      "Low-variance features removed (1): ['late_arrival_rate']\n",
      "No highly correlated feature pairs found (threshold: 0.90).\n",
      "\n",
      "Scaling applied (StandardScaler — fit on train only).\n",
      "Final pipeline: 44 -> 23 features\n",
      "\n",
      "Final feature list (23):\n",
      "   1. avg_working_hours\n",
      "   2. Age\n",
      "   3. TotalWorkingYears\n",
      "   4. YearsAtCompany\n",
      "   5. MonthlyIncome\n",
      "   6. YearsWithCurrManager\n",
      "   7. NumCompaniesWorked\n",
      "   8. DistanceFromHome\n",
      "   9. PercentSalaryHike\n",
      "  10. TrainingTimesLastYear\n",
      "  11. YearsSinceLastPromotion\n",
      "  12. BusinessTravel\n",
      "  13. MaritalStatus_Single\n",
      "  14. EnvironmentSatisfaction\n",
      "  15. JobSatisfaction\n",
      "  16. WorkLifeBalance\n",
      "  17. IncomePerJobLevel\n",
      "  18. ManagerStability\n",
      "  19. SatisfactionScore\n",
      "  20. PromotionStagnation\n",
      "  21. MaritalStatus_Married\n",
      "  22. JobRole_Manufacturing Director\n",
      "  23. EducationField_Technical Degree\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "2mq2penza1o",
   "metadata": {},
   "source": [
    "## Section 5: Class Imbalance — SMOTE\n",
    "\n",
    "Apply SMOTE **only on the training set** to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "id": "pybfgzicnhd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:38:21.132622200Z",
     "start_time": "2026-02-20T12:38:18.635544700Z"
    }
   },
   "source": [
    "# ==============================================================================\n",
    "# SMOTE OVERSAMPLING (train set only)\n",
    "# ==============================================================================\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Before SMOTE: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"After  SMOTE: {pd.Series(y_train_resampled).value_counts().to_dict()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: {0: 2959, 1: 569}\n",
      "After  SMOTE: {0: 2959, 1: 2959}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "6e876435",
   "metadata": {},
   "source": [
    "## Section 6: Export Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f12d22f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:38:21.485308800Z",
     "start_time": "2026-02-20T12:38:21.152622700Z"
    }
   },
   "source": [
    "# ==============================================================================\n",
    "# EXPORT\n",
    "# ==============================================================================\n",
    "import joblib\n",
    "\n",
    "# Save processed data\n",
    "X_train_resampled.to_csv(f\"{OUTPUT_DIR}/X_train.csv\", index=False)\n",
    "X_test_scaled.to_csv(f\"{OUTPUT_DIR}/X_test.csv\", index=False)\n",
    "pd.Series(y_train_resampled, name=\"Attrition\").to_csv(f\"{OUTPUT_DIR}/y_train.csv\", index=False)\n",
    "y_test.to_csv(f\"{OUTPUT_DIR}/y_test.csv\", index=False)\n",
    "\n",
    "# Also save non-SMOTE versions for fairness analysis and honest CV\n",
    "X_train_scaled.to_csv(f\"{OUTPUT_DIR}/X_train_no_smote.csv\", index=False)\n",
    "y_train.to_csv(f\"{OUTPUT_DIR}/y_train_no_smote.csv\", index=False)\n",
    "\n",
    "# Save pre-scaling train/test for fairness (unscaled binary columns)\n",
    "X_train.to_csv(f\"{OUTPUT_DIR}/X_train_unscaled.csv\", index=False)\n",
    "X_test.to_csv(f\"{OUTPUT_DIR}/X_test_unscaled.csv\", index=False)\n",
    "\n",
    "# Save scaler and imputer for reproducibility\n",
    "joblib.dump(scaler, f\"{OUTPUT_DIR}/scaler.joblib\")\n",
    "joblib.dump(imputer, f\"{OUTPUT_DIR}/imputer.joblib\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = list(X_train.columns)\n",
    "pd.Series(feature_names).to_csv(f\"{OUTPUT_DIR}/feature_names.csv\", index=False, header=False)\n",
    "\n",
    "print(f\"Exported to {OUTPUT_DIR}/:\")\n",
    "print(f\"  X_train.csv              ({X_train_resampled.shape}) — SMOTE + scaled\")\n",
    "print(f\"  X_test.csv               ({X_test_scaled.shape}) — scaled\")\n",
    "print(f\"  X_train_unscaled.csv     ({X_train.shape}) — for fairness analysis\")\n",
    "print(f\"  X_test_unscaled.csv      ({X_test.shape}) — for fairness analysis\")\n",
    "print(f\"  y_train.csv / y_test.csv\")\n",
    "print(f\"  scaler.joblib / imputer.joblib\")\n",
    "print(f\"  feature_names.csv        ({len(feature_names)} features)\")\n",
    "print(\"\\nPipeline: split -> impute (fit train) -> scale (fit train) -> SMOTE (train only)\")\n",
    "print(\"No data leakage.\")\n",
    "print(\"\\n-> Proceed to 04_Model_Benchmark.ipynb\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to C:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\\outputs/:\n",
      "  X_train.csv              ((5918, 23)) — SMOTE + scaled\n",
      "  X_test.csv               ((882, 23)) — scaled\n",
      "  X_train_unscaled.csv     ((3528, 23)) — for fairness analysis\n",
      "  X_test_unscaled.csv      ((882, 23)) — for fairness analysis\n",
      "  y_train.csv / y_test.csv\n",
      "  scaler.joblib / imputer.joblib\n",
      "  feature_names.csv        (23 features)\n",
      "\n",
      "Pipeline: split -> impute (fit train) -> scale (fit train) -> SMOTE (train only)\n",
      "No data leakage.\n",
      "\n",
      "-> Proceed to 04_Model_Benchmark.ipynb\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
