{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a41c888",
   "metadata": {},
   "source": [
    "# 03 — Feature Engineering & Preprocessing\n",
    "## HumanForYou — Employee Attrition Prediction\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "Transform raw merged data into **model-ready features**:\n",
    "1. Handle missing values with justified strategies\n",
    "2. Encode categorical variables (ordinal vs. one-hot)\n",
    "3. Engineer new features from existing ones\n",
    "4. Scale numerical features\n",
    "5. Address class imbalance (SMOTE)\n",
    "6. Export train/test splits for modeling\n",
    "\n",
    "> This notebook expects `merged_data.csv` from **01_Data_Validation_Pipeline**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef674950",
   "metadata": {},
   "source": [
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a505977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Only suppress expected warnings, not real errors\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Path Configuration (same logic as notebook 01) ---\n",
    "_cwd = Path.cwd()\n",
    "if (_cwd / \"data\" / \"raw\").exists():\n",
    "    PROJECT_ROOT = _cwd\n",
    "elif (_cwd.parent / \"data\" / \"raw\").exists():\n",
    "    PROJECT_ROOT = _cwd.parent\n",
    "else:\n",
    "    PROJECT_ROOT = Path(r\"c:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\")\n",
    "\n",
    "OUTPUT_DIR = str(PROJECT_ROOT / \"outputs\")\n",
    "\n",
    "df = pd.read_csv(f\"{OUTPUT_DIR}/merged_data.csv\")\n",
    "\n",
    "# Binary target\n",
    "df[\"Attrition\"] = (df[\"Attrition\"] == \"Yes\").astype(int)\n",
    "\n",
    "print(f\"Loaded: {df.shape[0]} rows x {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26e532",
   "metadata": {},
   "source": [
    "## Section 2: Feature Engineering & Encoding (before split)\n",
    "\n",
    "Features that don't require fitting on data (no leakage risk) are created before the split.\n",
    "Imputation happens **after** the split to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60770412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features created: ['IncomePerJobLevel', 'PromotionStagnation', 'SatisfactionScore', 'ManagerStability', 'LongHours']\n",
      "Categorical columns to encode: ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']\n",
      "Post-encoding shape: 4410 rows x 49 columns\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE ENGINEERING (safe before split — no fitted statistics)\n",
    "# ==============================================================================\n",
    "\n",
    "# Income per job level\n",
    "if \"MonthlyIncome\" in df.columns and \"JobLevel\" in df.columns:\n",
    "    df[\"IncomePerJobLevel\"] = df[\"MonthlyIncome\"] / df[\"JobLevel\"]\n",
    "\n",
    "# Promotion stagnation\n",
    "if \"YearsSinceLastPromotion\" in df.columns and \"YearsAtCompany\" in df.columns:\n",
    "    df[\"PromotionStagnation\"] = df[\"YearsSinceLastPromotion\"] / (df[\"YearsAtCompany\"] + 1)\n",
    "\n",
    "# Satisfaction composite score\n",
    "survey_items = [\"EnvironmentSatisfaction\", \"JobSatisfaction\", \"WorkLifeBalance\"]\n",
    "existing_items = [c for c in survey_items if c in df.columns]\n",
    "if existing_items:\n",
    "    df[\"SatisfactionScore\"] = df[existing_items].mean(axis=1)\n",
    "\n",
    "# Manager stability\n",
    "if \"YearsWithCurrManager\" in df.columns and \"YearsAtCompany\" in df.columns:\n",
    "    df[\"ManagerStability\"] = df[\"YearsWithCurrManager\"] / (df[\"YearsAtCompany\"] + 1)\n",
    "\n",
    "# Long hours proxy\n",
    "if \"avg_working_hours\" in df.columns:\n",
    "    df[\"LongHours\"] = (df[\"avg_working_hours\"] > 9).astype(int)\n",
    "\n",
    "new_features = [\"IncomePerJobLevel\", \"PromotionStagnation\", \"SatisfactionScore\", \"ManagerStability\", \"LongHours\"]\n",
    "new_features = [f for f in new_features if f in df.columns]\n",
    "print(f\"New features created: {new_features}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CATEGORICAL ENCODING (deterministic — no leakage)\n",
    "# ==============================================================================\n",
    "\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "print(f\"Categorical columns to encode: {cat_cols}\")\n",
    "\n",
    "# Ordinal encoding for BusinessTravel\n",
    "bt_map = {\"Non-Travel\": 0, \"Travel_Rarely\": 1, \"Travel_Frequently\": 2}\n",
    "if \"BusinessTravel\" in df.columns:\n",
    "    df[\"BusinessTravel\"] = df[\"BusinessTravel\"].map(bt_map)\n",
    "\n",
    "# One-hot encoding for remaining categoricals\n",
    "ohe_cols = [c for c in cat_cols if c != \"BusinessTravel\"]\n",
    "df = pd.get_dummies(df, columns=ohe_cols, drop_first=True, dtype=int)\n",
    "\n",
    "print(f\"Post-encoding shape: {df.shape[0]} rows x {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf719e6c",
   "metadata": {},
   "source": [
    "## Section 3: Train / Test Split\n",
    "\n",
    "Split **before** imputation to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47243ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3528 samples (16.1% attrition)\n",
      "Test set:  882 samples (16.1% attrition)\n",
      "Missing in train: 89\n",
      "Missing in test:  22\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TRAIN / TEST SPLIT (before imputation to avoid leakage)\n",
    "# ==============================================================================\n",
    "\n",
    "# Save and drop EmployeeID\n",
    "if \"EmployeeID\" in df.columns:\n",
    "    employee_ids = df[\"EmployeeID\"].copy()\n",
    "    df = df.drop(columns=[\"EmployeeID\"])\n",
    "\n",
    "X = df.drop(columns=[\"Attrition\"])\n",
    "y = df[\"Attrition\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({y_train.mean()*100:.1f}% attrition)\")\n",
    "print(f\"Test set:  {X_test.shape[0]} samples ({y_test.mean()*100:.1f}% attrition)\")\n",
    "print(f\"Missing in train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing in test:  {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756702d",
   "metadata": {},
   "source": [
    "## Section 4: Imputation (fit on train only)\n",
    "\n",
    "**Strategy**:\n",
    "- **KNN imputation** (k=5, distance-weighted) for all columns with missing values\n",
    "- Preserves local data structure and reduces imputation bias\n",
    "- Aligned with ethics document recommendation (avoid selection bias from median imputation)\n",
    "- **Fit on train only** → transform both train and test (no data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c646e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to impute: ['NumCompaniesWorked', 'TotalWorkingYears', 'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance']\n",
      "Imputation complete (median, fit on train only)\n",
      "Remaining NaN — train: 0, test: 0\n",
      "Scaling applied (StandardScaler — fit on train only).\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# IMPUTATION — KNNImputer (fit on train only, transform both)\n",
    "# ==============================================================================\n",
    "# Using KNNImputer instead of SimpleImputer(median) to:\n",
    "# - Preserve local data structure and reduce imputation bias\n",
    "# - Align with ethics document recommendation (avoid selection bias)\n",
    "\n",
    "# Identify columns with missing values\n",
    "cols_with_na = X_train.columns[X_train.isnull().any()].tolist()\n",
    "print(f\"Columns to impute: {cols_with_na}\")\n",
    "\n",
    "# KNNImputer — distance-weighted, fit on train only\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "X_train[cols_with_na] = imputer.fit_transform(X_train[cols_with_na])\n",
    "X_test[cols_with_na] = imputer.transform(X_test[cols_with_na])\n",
    "\n",
    "print(f\"✅ KNNImputer (k=5, distance-weighted) — consistent with ethics document\")\n",
    "print(f\"   Remaining NaN — train: {X_train.isnull().sum().sum()}, test: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665dbdd",
   "metadata": {},
   "source": [
    "## Section 4b: Feature Selection & Scaling\n",
    "\n",
    "After encoding, we have ~47 features including many one-hot columns. Pipeline:\n",
    "1. **Variance threshold** on **unscaled** data: remove near-constant features (variance < 0.01)\n",
    "2. **Scaling** (StandardScaler — fit on train only)\n",
    "3. **Correlation filter** on scaled data: remove features with |r| > 0.95 to reduce multicollinearity\n",
    "\n",
    "> **Note**: VarianceThreshold is applied **before** scaling because StandardScaler normalizes\n",
    "> all features to variance ≈ 1, which would make the filter useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE SELECTION & SCALING\n",
    "# ==============================================================================\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "n_before = X_train.shape[1]\n",
    "\n",
    "# 1) Remove near-constant features (variance < 0.01 on UNSCALED data)\n",
    "#    Must be done BEFORE scaling: StandardScaler sets variance ≈ 1 for all\n",
    "#    features, which would make this filter useless.\n",
    "var_selector = VarianceThreshold(threshold=0.01)\n",
    "var_selector.fit(X_train)\n",
    "low_var_cols = X_train.columns[~var_selector.get_support()].tolist()\n",
    "\n",
    "if low_var_cols:\n",
    "    print(f\"Low-variance features removed ({len(low_var_cols)}): {low_var_cols}\")\n",
    "    X_train = X_train.drop(columns=low_var_cols)\n",
    "    X_test = X_test.drop(columns=low_var_cols)\n",
    "else:\n",
    "    print(\"No low-variance features found.\")\n",
    "\n",
    "# 2) Scaling — fit on train only (AFTER variance filter)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled  = pd.DataFrame(scaler.transform(X_test),      columns=X_test.columns,  index=X_test.index)\n",
    "\n",
    "print(\"Scaling applied (StandardScaler — fit on train only).\")\n",
    "\n",
    "# 3) Remove highly correlated features (|r| > 0.95)\n",
    "corr_matrix = X_train_scaled.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_cols = [col for col in upper_tri.columns if any(upper_tri[col] > 0.95)]\n",
    "\n",
    "if high_corr_cols:\n",
    "    print(f\"Highly correlated features removed ({len(high_corr_cols)}): {high_corr_cols}\")\n",
    "    X_train_scaled = X_train_scaled.drop(columns=high_corr_cols)\n",
    "    X_test_scaled = X_test_scaled.drop(columns=high_corr_cols)\n",
    "    X_train = X_train.drop(columns=high_corr_cols)\n",
    "    X_test = X_test.drop(columns=high_corr_cols)\n",
    "else:\n",
    "    print(\"No highly correlated feature pairs found (threshold: 0.95).\")\n",
    "\n",
    "n_after = X_train_scaled.shape[1]\n",
    "print(f\"\\nFeature selection: {n_before} -> {n_after} features ({n_before - n_after} removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2mq2penza1o",
   "metadata": {},
   "source": [
    "## Section 5: Class Imbalance — SMOTE\n",
    "\n",
    "Apply SMOTE **only on the training set** to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pybfgzicnhd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SMOTE OVERSAMPLING (train set only)\n",
    "# ==============================================================================\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Before SMOTE: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"After  SMOTE: {pd.Series(y_train_resampled).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e876435",
   "metadata": {},
   "source": [
    "## Section 6: Export Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f12d22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to c:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\\outputs/:\n",
      "  X_train.csv              ((5918, 47)) — SMOTE + scaled\n",
      "  X_test.csv               ((882, 47)) — scaled\n",
      "  X_train_unscaled.csv     ((3528, 47)) — for fairness analysis\n",
      "  X_test_unscaled.csv      ((882, 47)) — for fairness analysis\n",
      "  y_train.csv / y_test.csv\n",
      "  scaler.joblib / imputer.joblib\n",
      "  feature_names.csv        (47 features)\n",
      "\n",
      "Pipeline: split -> impute (fit train) -> scale (fit train) -> SMOTE (train only)\n",
      "No data leakage.\n",
      "\n",
      "-> Proceed to 04_Model_Benchmark.ipynb\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EXPORT\n",
    "# ==============================================================================\n",
    "import joblib\n",
    "\n",
    "# Save processed data\n",
    "X_train_resampled.to_csv(f\"{OUTPUT_DIR}/X_train.csv\", index=False)\n",
    "X_test_scaled.to_csv(f\"{OUTPUT_DIR}/X_test.csv\", index=False)\n",
    "pd.Series(y_train_resampled, name=\"Attrition\").to_csv(f\"{OUTPUT_DIR}/y_train.csv\", index=False)\n",
    "y_test.to_csv(f\"{OUTPUT_DIR}/y_test.csv\", index=False)\n",
    "\n",
    "# Also save non-SMOTE versions for fairness analysis and honest CV\n",
    "X_train_scaled.to_csv(f\"{OUTPUT_DIR}/X_train_no_smote.csv\", index=False)\n",
    "y_train.to_csv(f\"{OUTPUT_DIR}/y_train_no_smote.csv\", index=False)\n",
    "\n",
    "# Save pre-scaling train/test for fairness (unscaled binary columns)\n",
    "X_train.to_csv(f\"{OUTPUT_DIR}/X_train_unscaled.csv\", index=False)\n",
    "X_test.to_csv(f\"{OUTPUT_DIR}/X_test_unscaled.csv\", index=False)\n",
    "\n",
    "# Save scaler and imputer for reproducibility\n",
    "joblib.dump(scaler, f\"{OUTPUT_DIR}/scaler.joblib\")\n",
    "joblib.dump(imputer, f\"{OUTPUT_DIR}/imputer.joblib\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = list(X_train.columns)\n",
    "pd.Series(feature_names).to_csv(f\"{OUTPUT_DIR}/feature_names.csv\", index=False, header=False)\n",
    "\n",
    "print(f\"Exported to {OUTPUT_DIR}/:\")\n",
    "print(f\"  X_train.csv              ({X_train_resampled.shape}) — SMOTE + scaled\")\n",
    "print(f\"  X_test.csv               ({X_test_scaled.shape}) — scaled\")\n",
    "print(f\"  X_train_unscaled.csv     ({X_train.shape}) — for fairness analysis\")\n",
    "print(f\"  X_test_unscaled.csv      ({X_test.shape}) — for fairness analysis\")\n",
    "print(f\"  y_train.csv / y_test.csv\")\n",
    "print(f\"  scaler.joblib / imputer.joblib\")\n",
    "print(f\"  feature_names.csv        ({len(feature_names)} features)\")\n",
    "print(\"\\nPipeline: split -> impute (fit train) -> scale (fit train) -> SMOTE (train only)\")\n",
    "print(\"No data leakage.\")\n",
    "print(\"\\n-> Proceed to 04_Model_Benchmark.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}