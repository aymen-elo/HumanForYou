{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 — Model Optimization & Final Selection",
    "## HumanForYou — Employee Attrition Prediction",
    "",
    "---",
    "",
    "### Objective",
    "",
    "**Fine-tune the top model(s)** from the benchmark to maximize predictive performance:",
    "1. Hyperparameter tuning via GridSearchCV / RandomizedSearchCV",
    "2. Threshold calibration to optimize the Precision-Recall trade-off",
    "3. Feature importance analysis (SHAP for explainability)",
    "4. Fairness evaluation on sensitive variables",
    "5. Final model selection and business recommendations",
    "",
    "> **Adapted from** the confidence calibration methodology used in a previous detection project — same threshold optimization logic applied to classification.",
    "",
    "> This notebook expects preprocessed data from **03** and benchmark results from **04**."
   ],
   "id": "8bd63397"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup"
   ],
   "id": "ea67bcdc"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# ==============================================================================\n# IMPORTS\n# ==============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport time\nimport joblib\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, classification_report, confusion_matrix,\n    roc_curve, precision_recall_curve, make_scorer\n)\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nplt.rcParams.update({\"figure.figsize\": (12, 6), \"figure.dpi\": 100})\nsns.set_theme(style=\"whitegrid\")\n\n# --- Path Configuration ---\n_cwd = Path.cwd()\nif (_cwd / \"data\" / \"raw\").exists():\n    PROJECT_ROOT = _cwd\nelif (_cwd.parent / \"data\" / \"raw\").exists():\n    PROJECT_ROOT = _cwd.parent\nelse:\n    PROJECT_ROOT = Path(r\"c:\\Users\\yanis\\Documents\\CESI\\A5\\AI Project\\HumanForYou\")\n\nOUTPUT_DIR = str(PROJECT_ROOT / \"outputs\")\n\n# Load data\nX_train = pd.read_csv(f\"{OUTPUT_DIR}/X_train.csv\")\nX_test  = pd.read_csv(f\"{OUTPUT_DIR}/X_test.csv\")\ny_train = pd.read_csv(f\"{OUTPUT_DIR}/y_train.csv\").squeeze()\ny_test  = pd.read_csv(f\"{OUTPUT_DIR}/y_test.csv\").squeeze()\n\n# Non-SMOTE for fair CV\nX_train_ns = pd.read_csv(f\"{OUTPUT_DIR}/X_train_no_smote.csv\")\ny_train_ns = pd.read_csv(f\"{OUTPUT_DIR}/y_train_no_smote.csv\").squeeze()\n\n# Load benchmark results to pick top models\nbenchmark = pd.read_csv(f\"{OUTPUT_DIR}/model_benchmark_results.csv\")\nprint(\"Top 3 from benchmark:\")\nprint(benchmark.head(3)[[\"Model\", \"F1-Score\", \"AUC-ROC\"]].to_string(index=False))",
   "outputs": [],
   "id": "927fd643"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Hyperparameter Tuning",
    "",
    "Tune the top candidate models using GridSearchCV with stratified 5-fold cross-validation.",
    "",
    "> **Note**: We tune on non-SMOTE data with `class_weight='balanced'` to let the model handle imbalance natively during CV."
   ],
   "id": "26941e7c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# ==============================================================================\n# HYPERPARAMETER GRIDS\n# ==============================================================================\n\nTUNING_CONFIGS = {\n    \"Random Forest\": {\n        \"model\": RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=\"balanced\"),\n        \"params\": {\n            \"n_estimators\": [100, 200, 300],\n            \"max_depth\": [10, 15, 20, None],\n            \"min_samples_split\": [2, 5, 10],\n            \"min_samples_leaf\": [1, 2, 4],\n        }\n    },\n    \"XGBoost\": {\n        \"model\": XGBClassifier(random_state=42, eval_metric=\"logloss\", verbosity=0),\n        \"params\": {\n            \"n_estimators\": [100, 200, 300],\n            \"max_depth\": [3, 5, 7, 9],\n            \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n            \"subsample\": [0.7, 0.8, 1.0],\n            \"scale_pos_weight\": [1, 3, 5],\n        }\n    },\n    \"Gradient Boosting\": {\n        \"model\": GradientBoostingClassifier(random_state=42),\n        \"params\": {\n            \"n_estimators\": [100, 200, 300],\n            \"max_depth\": [3, 5, 7],\n            \"learning_rate\": [0.01, 0.05, 0.1],\n            \"min_samples_split\": [2, 5, 10],\n            \"subsample\": [0.7, 0.8, 1.0],\n        }\n    },\n}\n\nprint(f\"Tuning {len(TUNING_CONFIGS)} models...\")\nfor name, cfg in TUNING_CONFIGS.items():\n    n_combos = np.prod([len(v) for v in cfg[\"params\"].values()])\n    print(f\"  {name}: {n_combos} parameter combinations\")",
   "outputs": [],
   "id": "6d3a1a1c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# GRID SEARCH / RANDOMIZED SEARCH\n",
    "# ==============================================================================\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "tuned_models = {}\n",
    "\n",
    "for name, cfg in TUNING_CONFIGS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Tuning: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    n_combos = np.prod([len(v) for v in cfg[\"params\"].values()])\n",
    "    \n",
    "    # Use RandomizedSearchCV if search space is large\n",
    "    if n_combos > 200:\n",
    "        search = RandomizedSearchCV(\n",
    "            cfg[\"model\"], cfg[\"params\"], n_iter=50,\n",
    "            scoring=\"f1\", cv=cv, n_jobs=-1, random_state=42, verbose=0\n",
    "        )\n",
    "        search_type = \"RandomizedSearchCV (50 iters)\"\n",
    "    else:\n",
    "        search = GridSearchCV(\n",
    "            cfg[\"model\"], cfg[\"params\"],\n",
    "            scoring=\"f1\", cv=cv, n_jobs=-1, verbose=0\n",
    "        )\n",
    "        search_type = \"GridSearchCV\"\n",
    "    \n",
    "    t0 = time.time()\n",
    "    search.fit(X_train_ns, y_train_ns)\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    tuned_models[name] = search.best_estimator_\n",
    "    \n",
    "    print(f\"  Search: {search_type}\")\n",
    "    print(f\"  Best F1 (CV): {search.best_score_:.4f}\")\n",
    "    print(f\"  Best params:  {search.best_params_}\")\n",
    "    print(f\"  Time: {elapsed:.1f}s\")"
   ],
   "outputs": [],
   "id": "bac63289"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Threshold Calibration",
    "",
    "For each tuned model, sweep classification thresholds to find the **optimal operating point** that balances Precision and Recall for HumanForYou's business needs.",
    "",
    "> In HR attrition, **Recall is slightly more valuable** than Precision — missing an at-risk employee is more costly than a false alert."
   ],
   "id": "d8aeaa1a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# THRESHOLD CALIBRATION\n",
    "# ==============================================================================\n",
    "\n",
    "def calibrate_threshold(model, X_test, y_test, name, target_recall=0.75):\n",
    "    \"\"\"\n",
    "    Sweep thresholds and find the optimal operating point.\n",
    "    Priority: maximize F1, with a minimum recall constraint.\n",
    "    \"\"\"\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    thresholds = np.arange(0.1, 0.91, 0.01)\n",
    "    records = []\n",
    "    \n",
    "    for t in thresholds:\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "        rec  = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1   = f1_score(y_test, y_pred, zero_division=0)\n",
    "        records.append({\"threshold\": t, \"precision\": prec, \"recall\": rec, \"f1\": f1})\n",
    "    \n",
    "    df_cal = pd.DataFrame(records)\n",
    "    \n",
    "    # Find best threshold: maximize F1 where recall >= target\n",
    "    candidates = df_cal[df_cal[\"recall\"] >= target_recall]\n",
    "    if len(candidates) > 0:\n",
    "        best_row = candidates.loc[candidates[\"f1\"].idxmax()]\n",
    "    else:\n",
    "        best_row = df_cal.loc[df_cal[\"f1\"].idxmax()]\n",
    "    \n",
    "    return df_cal, best_row\n",
    "\n",
    "calibration_results = {}\n",
    "fig, axes = plt.subplots(1, len(tuned_models), figsize=(6 * len(tuned_models), 5))\n",
    "if len(tuned_models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (name, model) in enumerate(tuned_models.items()):\n",
    "    df_cal, best = calibrate_threshold(model, X_test, y_test, name)\n",
    "    calibration_results[name] = {\"table\": df_cal, \"best\": best}\n",
    "    \n",
    "    axes[i].plot(df_cal[\"threshold\"], df_cal[\"precision\"], label=\"Precision\", linewidth=2)\n",
    "    axes[i].plot(df_cal[\"threshold\"], df_cal[\"recall\"], label=\"Recall\", linewidth=2)\n",
    "    axes[i].plot(df_cal[\"threshold\"], df_cal[\"f1\"], label=\"F1-Score\", linewidth=2, linestyle=\"--\")\n",
    "    axes[i].axvline(best[\"threshold\"], color=\"red\", linestyle=\":\", alpha=0.7,\n",
    "                    label=f\"Optimal: {best['threshold']:.2f}\")\n",
    "    axes[i].set_xlabel(\"Threshold\")\n",
    "    axes[i].set_ylabel(\"Score\")\n",
    "    axes[i].set_title(f\"{name}\", fontweight=\"bold\")\n",
    "    axes[i].legend(fontsize=9)\n",
    "    axes[i].set_xlim(0.1, 0.9)\n",
    "    axes[i].set_ylim(0, 1.02)\n",
    "    \n",
    "    print(f\"{name} — Optimal threshold: {best['threshold']:.2f}\")\n",
    "    print(f\"  Precision: {best['precision']:.4f}  Recall: {best['recall']:.4f}  F1: {best['f1']:.4f}\")\n",
    "\n",
    "plt.suptitle(\"Threshold Calibration — Precision / Recall Trade-off\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/threshold_calibration.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "id": "9f459a80"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Feature Importance & Explainability",
    "",
    "Use both built-in feature importances and SHAP values for interpretability.",
    "",
    "> **ALTAI Requirement 4 (Transparency)**: The model must be explainable to HR stakeholders."
   ],
   "id": "bb1096ec"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# FEATURE IMPORTANCE — Built-in (Tree models)\n",
    "# ==============================================================================\n",
    "\n",
    "# Pick the best tuned model for detailed analysis\n",
    "best_name = max(calibration_results, key=lambda k: calibration_results[k][\"best\"][\"f1\"])\n",
    "best_model = tuned_models[best_name]\n",
    "print(f\"Detailed analysis on: {best_name}\")\n",
    "\n",
    "# Feature importance (if tree-based)\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    importances = pd.Series(best_model.feature_importances_, index=X_train.columns)\n",
    "    top20 = importances.nlargest(20)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top20.sort_values().plot(kind=\"barh\", ax=ax, color=\"#3498db\")\n",
    "    ax.set_xlabel(\"Feature Importance\")\n",
    "    ax.set_title(f\"Top 20 Feature Importances — {best_name}\", fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/feature_importance.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "id": "dd131b56"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# SHAP VALUES — Explainability\n",
    "# ==============================================================================\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    # Use a sample for speed\n",
    "    X_sample = X_test.sample(min(500, len(X_test)), random_state=42)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # If binary classification, take class 1 SHAP values\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_vals = shap_values[1]\n",
    "    else:\n",
    "        shap_vals = shap_values\n",
    "    \n",
    "    # Summary plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_vals, X_sample, show=False, max_display=20)\n",
    "    plt.title(f\"SHAP Summary — {best_name}\", fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/shap_summary.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"SHAP analysis complete — see plot for feature impact directions.\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")\n",
    "    print(\"Falling back to built-in feature importance only.\")"
   ],
   "outputs": [],
   "id": "15819b7b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Fairness Evaluation",
    "",
    "**ALTAI Requirement 5**: Check for discriminatory bias in predictions across sensitive groups."
   ],
   "id": "6feba250"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# ==============================================================================\n# FAIRNESS METRICS — Using unscaled data for correct group identification\n# ==============================================================================\n\n# Load unscaled data (binary 0/1 columns, not standardized floats)\nimport os\nunscaled_test_path = f\"{OUTPUT_DIR}/X_test_unscaled.csv\"\nif os.path.exists(unscaled_test_path):\n    X_test_unscaled = pd.read_csv(unscaled_test_path)\n    print(\"Using unscaled test data for fairness analysis.\")\nelse:\n    # Fallback: use scaled data with rounding for binary columns\n    X_test_unscaled = X_test.copy()\n    print(\"WARNING: unscaled data not found, using scaled data (may be inaccurate).\")\n\n# Find sensitive columns\ngender_cols = [c for c in X_test_unscaled.columns if \"Gender\" in c]\nmarital_cols = [c for c in X_test_unscaled.columns if \"MaritalStatus\" in c]\n\nprint(\"\\nFAIRNESS EVALUATION\")\nprint(\"=\" * 65)\n\nbest_threshold = calibration_results[best_name][\"best\"][\"threshold\"]\ny_proba = best_model.predict_proba(X_test)[:, 1]\ny_pred = (y_proba >= best_threshold).astype(int)\n\ndef fairness_report(X_unscaled, y_true, y_pred, group_col, group_name):\n    \"\"\"Compute per-group metrics for fairness analysis.\"\"\"\n    print(f\"\\n  {group_name} (column: {group_col})\")\n    print(f\"  {'-'*50}\")\n    \n    groups = X_unscaled[group_col].unique()\n    rates = {}\n    for g in sorted(groups):\n        mask = X_unscaled[group_col] == g\n        n = mask.sum()\n        if n == 0:\n            continue\n        tp_rate = recall_score(y_true[mask.values], y_pred[mask.values], zero_division=0)\n        pred_pos_rate = y_pred[mask.values].mean()\n        rates[g] = {\"n\": n, \"recall\": tp_rate, \"pred_positive_rate\": pred_pos_rate}\n        label = f\"Group {g}\" if isinstance(g, (int, float)) else str(g)\n        print(f\"    {label} (n={n}): Recall={tp_rate:.3f}, Pred+ rate={pred_pos_rate:.3f}\")\n    \n    # Disparate impact\n    if len(rates) >= 2:\n        vals = [v[\"pred_positive_rate\"] for v in rates.values() if v[\"pred_positive_rate\"] > 0]\n        if len(vals) >= 2:\n            di = min(vals) / max(vals)\n            print(f\"    -> Disparate Impact: {di:.3f} {'OK (> 0.8)' if di > 0.8 else 'WARNING: potential bias (< 0.8)'}\")\n    return rates\n\nfor col in gender_cols:\n    fairness_report(X_test_unscaled, y_test, y_pred, col, \"Gender\")\n\nfor col in marital_cols[:1]:\n    fairness_report(X_test_unscaled, y_test, y_pred, col, \"Marital Status\")",
   "outputs": [],
   "id": "dba63751"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Final Model Export & Business Recommendations"
   ],
   "id": "cb360a77"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# FINAL MODEL — Save\n",
    "# ==============================================================================\n",
    "\n",
    "best_threshold = calibration_results[best_name][\"best\"][\"threshold\"]\n",
    "\n",
    "# Save model\n",
    "model_path = f\"{OUTPUT_DIR}/final_model.joblib\"\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "# Save operating point\n",
    "import json\n",
    "operating_point = {\n",
    "    \"model_name\": best_name,\n",
    "    \"optimal_threshold\": float(best_threshold),\n",
    "    \"test_metrics\": {\n",
    "        \"precision\": float(calibration_results[best_name][\"best\"][\"precision\"]),\n",
    "        \"recall\": float(calibration_results[best_name][\"best\"][\"recall\"]),\n",
    "        \"f1\": float(calibration_results[best_name][\"best\"][\"f1\"]),\n",
    "    },\n",
    "    \"best_params\": {str(k): str(v) for k, v in best_model.get_params().items()}\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/operating_point.json\", \"w\") as f:\n",
    "    json.dump(operating_point, f, indent=2)\n",
    "\n",
    "print(f\"Final model saved: {model_path}\")\n",
    "print(f\"Operating point: threshold = {best_threshold:.2f}\")\n",
    "print(f\"  Precision: {operating_point['test_metrics']['precision']:.4f}\")\n",
    "print(f\"  Recall:    {operating_point['test_metrics']['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {operating_point['test_metrics']['f1']:.4f}\")"
   ],
   "outputs": [],
   "id": "d7ee9bdd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# CLASSIFICATION REPORT — Final model at optimal threshold\n",
    "# ==============================================================================\n",
    "\n",
    "y_final = (best_model.predict_proba(X_test)[:, 1] >= best_threshold).astype(int)\n",
    "\n",
    "print(f\"FINAL CLASSIFICATION REPORT — {best_name} @ threshold={best_threshold:.2f}\")\n",
    "print(\"=\" * 65)\n",
    "print(classification_report(y_test, y_final, target_names=[\"Stay\", \"Leave\"]))\n",
    "\n",
    "# Confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_final)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[\"Stay\", \"Leave\"], yticklabels=[\"Stay\", \"Leave\"],\n",
    "            annot_kws={\"size\": 16})\n",
    "ax.set_ylabel(\"Actual\", fontsize=12)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=12)\n",
    "ax.set_title(f\"Final Model — {best_name} (threshold={best_threshold:.2f})\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/final_confusion_matrix.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "id": "6d78f771"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# BUSINESS RECOMMENDATIONS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"BUSINESS RECOMMENDATIONS FOR HUMANFORYOU\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    top5 = importances.nlargest(5)\n",
    "    print(f\"\"\"\n",
    "Based on {best_name} analysis, the top attrition risk factors are:\n",
    "\n",
    "{chr(10).join(f\"  {i+1}. {feat} (importance: {imp:.4f})\" for i, (feat, imp) in enumerate(top5.items()))}\n",
    "\n",
    "RECOMMENDED ACTIONS:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "1. EARLY WARNING SYSTEM\n",
    "   Deploy this model monthly to score all employees.\n",
    "   Flag those with predicted attrition probability > {best_threshold:.0%} for HR review.\n",
    "\n",
    "2. TARGETED RETENTION\n",
    "   Focus retention efforts on the top risk factors identified above.\n",
    "   Design specific programs addressing each factor.\n",
    "\n",
    "3. MONITORING & GOVERNANCE\n",
    "   Retrain the model annually with fresh data.\n",
    "   Monitor fairness metrics quarterly (disparate impact > 0.8).\n",
    "   Maintain human oversight: model flags → HR interview → decision.\n",
    "\n",
    "4. ETHICAL SAFEGUARDS\n",
    "   Never use the model as sole basis for employment decisions.\n",
    "   Ensure transparency: employees can request explanation of their risk score.\n",
    "   Regular bias audits per ALTAI guidelines.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ Project pipeline complete.\")\n",
    "print(\"  Deliverables generated in:\", OUTPUT_DIR)"
   ],
   "outputs": [],
   "id": "1bd0cbc4"
  }
 ]
}